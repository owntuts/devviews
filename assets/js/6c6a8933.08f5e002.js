"use strict";(self.webpackChunkinterviewdev=self.webpackChunkinterviewdev||[]).push([[8750,970,7591,7571,2526,9241,2439,5932,6312],{3905:function(e,t,a){a.d(t,{Zo:function(){return u},kt:function(){return d}});var r=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=r.createContext({}),p=function(e){var t=r.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},k=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),c=p(a),k=n,d=c["".concat(l,".").concat(k)]||c[k]||m[k]||o;return a?r.createElement(d,i(i({ref:t},u),{},{components:a})):r.createElement(d,i({ref:t},u))}));function d(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=a.length,i=new Array(o);i[0]=k;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:n,i[1]=s;for(var p=2;p<o;p++)i[p]=a[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}k.displayName="MDXCreateElement"},4312:function(e,t,a){a.d(t,{ZP:function(){return s}});var r=a(3117),n=(a(7294),a(3905));const o={toc:[]},i="wrapper";function s(e){let{components:t,...a}=e;return(0,n.kt)(i,(0,r.Z)({},o,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("div",{className:"interview"},a.level&&(0,n.kt)("span",{className:"level"},a.level),a.children))}s.isMDXComponent=!0},3582:function(e,t,a){a.r(t),a.d(t,{assets:function(){return h},contentTitle:function(){return d},default:function(){return w},frontMatter:function(){return k},metadata:function(){return f},toc:function(){return g}});var r=a(3117),n=(a(7294),a(3905)),o=(a(4312),a(4504)),i=a(1145),s=a(2793),l=a(6398),p=a(7811),u=a(9491),c=a(5008),m=a(8258);const k={sidebar_position:3,id:"kafka-from-zero-to-hero",sidebar_label:"Kafka From Zero To Hero",title:"Kafka From Zero To Hero",tags:["Angular Interviews"]},d="Kafka From Zero To Hero",f={unversionedId:"kafka/kafka-from-zero-to-hero",id:"kafka/kafka-from-zero-to-hero",title:"Kafka From Zero To Hero",description:"Kafka Architecture",source:"@site/docs/kafka/1.intro.md",sourceDirName:"kafka",slug:"/kafka/kafka-from-zero-to-hero",permalink:"/devviews/interviews/kafka/kafka-from-zero-to-hero",draft:!1,editUrl:"https://github.com/owntuts/devviews/edit/main/docs/kafka/1.intro.md",tags:[{label:"Angular Interviews",permalink:"/devviews/interviews/tags/angular-interviews"}],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,id:"kafka-from-zero-to-hero",sidebar_label:"Kafka From Zero To Hero",title:"Kafka From Zero To Hero",tags:["Angular Interviews"]},sidebar:"kafkaInterviewSidebar",previous:{title:"Kafka Zookeeper Roles",permalink:"/devviews/interviews/kafka/hero/zookeeper"}},h={},g=[{value:"Kafka Architecture",id:"kafka-architecture",level:3},{value:"Data Distribution &amp; Replication",id:"data-distribution--replication",level:3},{value:"Kafka Topic &amp; Partitioning",id:"kafka-topic--partitioning",level:3},{value:"Zookeeper Roles",id:"zookeeper-roles",level:3},{value:"Implement Kafka Cluster",id:"implement-kafka-cluster",level:3},{value:"Pub/Sub Example in Java",id:"pubsub-example-in-java",level:3},{value:"Kstream vs Ktable",id:"kstream-vs-ktable",level:3},{value:"Common Stream Operations",id:"common-stream-operations",level:3}],v={toc:g},b="wrapper";function w(e){let{components:t,...a}=e;return(0,n.kt)(b,(0,r.Z)({},v,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"kafka-from-zero-to-hero"},"Kafka From Zero To Hero"),(0,n.kt)("h3",{id:"kafka-architecture"},"Kafka Architecture"),(0,n.kt)(o.default,{mdxType:"Architecture"}),(0,n.kt)("h3",{id:"data-distribution--replication"},"Data Distribution & Replication"),(0,n.kt)(s.default,{mdxType:"Replication"}),(0,n.kt)("h3",{id:"kafka-topic--partitioning"},"Kafka Topic & Partitioning"),(0,n.kt)(i.default,{mdxType:"Partition"}),(0,n.kt)("h3",{id:"zookeeper-roles"},"Zookeeper Roles"),(0,n.kt)(p.default,{mdxType:"Zookeeper"}),(0,n.kt)("h3",{id:"implement-kafka-cluster"},"Implement Kafka Cluster"),(0,n.kt)(l.default,{mdxType:"ImplementCluster"}),(0,n.kt)("h3",{id:"pubsub-example-in-java"},"Pub/Sub Example in Java"),(0,n.kt)(u.default,{mdxType:"PubSubExample"}),(0,n.kt)("h3",{id:"kstream-vs-ktable"},"Kstream vs Ktable"),(0,n.kt)(c.default,{mdxType:"KstreamvsKtable"}),(0,n.kt)("h3",{id:"common-stream-operations"},"Common Stream Operations"),(0,n.kt)(m.default,{mdxType:"CommonStream"}))}w.isMDXComponent=!0},4504:function(e,t,a){a.r(t),a.d(t,{assets:function(){return l},contentTitle:function(){return i},default:function(){return m},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return p}});var r=a(3117),n=(a(7294),a(3905));const o={sidebar_position:1e3,sidebar_label:"Kafka Architecture",title:"Kafka Architecture",tags:["Kafka Knowledge"]},i=void 0,s={unversionedId:"kafka/hero/architecture",id:"kafka/hero/architecture",title:"Kafka Architecture",description:"Architecture",source:"@site/docs/kafka/hero/architecture.md",sourceDirName:"kafka/hero",slug:"/kafka/hero/architecture",permalink:"/devviews/interviews/kafka/hero/architecture",draft:!1,editUrl:"https://github.com/owntuts/devviews/edit/main/docs/kafka/hero/architecture.md",tags:[{label:"Kafka Knowledge",permalink:"/devviews/interviews/tags/kafka-knowledge"}],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3,sidebar_label:"Kafka Architecture",title:"Kafka Architecture",tags:["Kafka Knowledge"]},sidebar:"kafkaInterviewSidebar",next:{title:"list of common stream operations",permalink:"/devviews/interviews/kafka/hero/common-stream"}},l={},p=[],u={toc:p},c="wrapper";function m(e){let{components:t,...o}=e;return(0,n.kt)(c,(0,r.Z)({},u,o,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("details",{open:!0},(0,n.kt)("summary",null,(0,n.kt)("h5",null,"Architecture")),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Kafka Architecture",src:a(5841).Z,width:"947",height:"486"})),(0,n.kt)("p",null,"Kafka is a distributed messaging system that can be used for real-time data streaming and processing. It is highly scalable and fault-tolerant, making it a popular choice for handling large volumes of data."),(0,n.kt)("p",null,"The Kafka architecture is composed of four main components:"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Producers"),": A producer is responsible for sending messages to Kafka topics. It can be a standalone application that generates events or a system that collects log data from various sources."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Brokers"),": Kafka brokers are the nodes that store and manage the messages that are produced by the producers. They are responsible for maintaining the replicas of the data and distributing the messages to consumers."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Consumers"),": A consumer is responsible for subscribing to the Kafka topics, receiving and processing the messages. It can be a standalone application, a service or a system that processes events in real-time."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Zookeeper"),": Zookeeper is responsible for managing the Kafka cluster and the coordination of the brokers, producers and consumers. It provides features like leader election, configuration management, and synchronization."),(0,n.kt)("p",null,"In the above diagram, the producers generate messages and send them to the Kafka cluster. The cluster is composed of the brokers, which are responsible for storing and managing the messages. The Kafka cluster is managed by Zookeeper, which coordinates communication between the brokers, producers and consumers."),(0,n.kt)("p",null,"Consumers subscribe to specific Kafka topics and receive messages from the brokers. They can process the data and perform real-time analytics. The Kafka architecture allows for high scalability, low latency and fault tolerance, making it a popular choice for handling large volumes of data.")))}m.isMDXComponent=!0},8258:function(e,t,a){a.r(t),a.d(t,{assets:function(){return l},contentTitle:function(){return i},default:function(){return m},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return p}});var r=a(3117),n=(a(7294),a(3905));const o={sidebar_position:1e3,sidebar_label:"list of common stream operations",title:"list of common stream operations",tags:["Kafka Knowledge"]},i=void 0,s={unversionedId:"kafka/hero/common-stream",id:"kafka/hero/common-stream",title:"list of common stream operations",description:"List of common stream operations",source:"@site/docs/kafka/hero/common-stream.md",sourceDirName:"kafka/hero",slug:"/kafka/hero/common-stream",permalink:"/devviews/interviews/kafka/hero/common-stream",draft:!1,editUrl:"https://github.com/owntuts/devviews/edit/main/docs/kafka/hero/common-stream.md",tags:[{label:"Kafka Knowledge",permalink:"/devviews/interviews/tags/kafka-knowledge"}],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3,sidebar_label:"list of common stream operations",title:"list of common stream operations",tags:["Kafka Knowledge"]},sidebar:"kafkaInterviewSidebar",previous:{title:"Kafka Architecture",permalink:"/devviews/interviews/kafka/hero/architecture"},next:{title:"Kafka Implement Cluster",permalink:"/devviews/interviews/kafka/hero/implement-cluster"}},l={},p=[],u={toc:p},c="wrapper";function m(e){let{components:t,...a}=e;return(0,n.kt)(c,(0,r.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("details",{open:!0},(0,n.kt)("summary",null,(0,n.kt)("h5",null,"List of common stream operations")),(0,n.kt)("p",null,"Kafka Streams provides a high-level DSL (Domain Specific Language) that lets you define common stream processing operations such as ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"filtering, mapping, joining, or aggregating"))," on data streams."),(0,n.kt)("p",null,"Here are some examples of these operations:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"filter"),": Filters records from a stream or table based on a predicate function. For example, if we have a stream of messages with keys and values as strings, we can filter out records that have a null value or a negative key:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Create a stream from a Kafka topic\nKStream<String, String> input = builder.stream("input-topic");\n\n// Filter out records with null value or negative key\nKStream<String, String> output = input.filter((key, value) -> value != null && !key.startsWith("-"));\n\n// Send the output stream to another Kafka topic\noutput.to("output-topic");\n')),(0,n.kt)("p",null,"The input and output of this operation are both streams."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"map"),": Transforms each record from a stream or table by applying a mapper function. For example, if we have a stream of messages with keys and values as strings, we can map a record's value to its length or its key to its hash code:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Create a stream from a Kafka topic\nKStream<String, String> input = builder.stream("input-topic");\n\n// Map the value to its length and the key to its hash code\nKStream<Integer, Integer> output = input.map((key, value) -> new KeyValue<>(key.hashCode(), value.length()));\n\n// Send the output stream to another Kafka topic\noutput.to("output-topic");\n')),(0,n.kt)("p",null,"The input and output of this operation are both streams."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"join"),": Joins two streams or tables based on a common key and a joiner function. For example, if we have a stream of customer orders and a table of customer profiles, we can join them by customer ID and enrich the order records with profile information:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Create a stream from a Kafka topic\nKStream<String, Order> orders = builder.stream("orders-topic");\n\n// Create a table from another Kafka topic\nKTable<String, Profile> profiles = builder.table("profiles-topic");\n\n// Join the stream and the table by customer ID and add profile information to order records\nKStream<String, EnrichedOrder> output = orders.join(profiles,\n    (order, profile) -> new EnrichedOrder(order.getId(), order.getAmount(), profile.getName(), profile.getAddress()));\n\n// Send the output stream to another Kafka topic\noutput.to("output-topic");\n')),(0,n.kt)("p",null,"The input of this operation is a stream and a table, and the output is a stream."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"aggregate"),": Aggregates records from a stream or table by grouping them by key and applying an aggregator function. For example, if we have a stream of clicks by user ID and URL, we can aggregate them by user ID and count the number of clicks per user:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Create a stream from a Kafka topic\nKStream<String, Click> clicks = builder.stream("clicks-topic");\n\n// Aggregate the clicks by user ID and count the number of clicks per user\nKTable<String, Long> output = clicks.groupByKey().count();\n\n// Send the output table to another Kafka topic\noutput.toStream().to("output-topic");\n')),(0,n.kt)("p",null,"The input of this operation is a stream, and the output is a table."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("inlineCode",{parentName:"strong"},"join(KStream, ValueJoiner, JoinWindows)")),": This operation joins two KStreams based on their keys and a window specification. It returns a new KStream that contains pairs of values from both streams for each matching key within the window. For example, if you have two streams of user clicks and purchases, you can join them to get pairs of clicks and purchases for each user within a certain time interval.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Assume we have two KStreams of user clicks and purchases\nKStream<String, String> clicks = ...; // key: user, value: click\nKStream<String, String> purchases = ...; // key: user, value: purchase\n\n// Define a join window of 10 seconds\nJoinWindows joinWindow = JoinWindows.of(Duration.ofSeconds(10));\n\n// Define a value joiner that concatenates the values\nValueJoiner<String, String, String> valueJoiner = (click, purchase) -> click + " -> " + purchase;\n\n// Join the two streams on the user key and print the result\nKStream<String, String> joined = clicks.join(purchases, valueJoiner, joinWindow);\njoined.foreach((user, value) -> System.out.println(user + ": " + value));\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"Input:\n  clicks: (user1, click1), (user2, click2), (user1, click3), (user3, click4)\n  purchases: (user1, purchase1), (user2, purchase2), (user3, purchase3), (user1, purchase4)\nOutput:\n  user1: click1 -> purchase1 user2: click2 -> purchase2 user1: click3 -> purchase4 user3: click4 -> purchase3\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("inlineCode",{parentName:"strong"},"groupBy")),": This operation splits a stream into sub-streams based on a key function. It returns a KGroupedStream object that can be further aggregated or windowed. For example, if you have a stream of words, you can group them by their length to get sub-streams of words with the same length.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Assume we have a KStream of words\nKStream<String, String> words = ...; // key: null, value: word\n\n// Define a key function that returns the word length\nKeyValueMapper<String, String, Integer> keyFunction = (key, word) -> word.length();\n\n// Group the words by their length and print the result\nKGroupedStream<Integer, String> grouped = words.groupBy(keyFunction);\ngrouped.foreach((length, word) -> System.out.println(length + ": " + word));\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'Input:\n  words: "hello", "world", "kafka", "streams", "java"\nOutput:\n  grouped: 5 -> ("hello", "world", "kafka"), 6 -> ("streams"), 4 -> ("java")\n')),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("inlineCode",{parentName:"strong"},"reduce")),": This operation applies a reducer function to each sub-stream of a KGroupedStream and returns a KTable that contains the latest reduced value for each key. For example, if you have a KGroupedStream of user ratings for movies, you can reduce them to get the average rating for each movie\xb3.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Assume we have a KGroupedStream of user ratings for movies\nKGroupedStream<String, Integer> ratings = ...; // key: movie, value: rating\n\n// Define a reducer function that computes the average rating\nReducer<Integer> reducer = (oldValue, newValue) -> (oldValue + newValue) / 2;\n\n// Reduce the ratings by movie and print the result\nKTable<String, Integer> reduced = ratings.reduce(reducer);\nreduced.toStream().foreach((movie, rating) -> System.out.println(movie + ": " + rating));\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"Input:\n  ratings: movie1 -> (user1, 4), movie2 -> (user2, 5), movie1 -> (user3, 3), movie2 -> (user4, 4), movie1 -> (user5, 5)\nOutput:\n  reduced: movie1 -> 4, movie2 -> 4.5\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("inlineCode",{parentName:"strong"},"windowedBy")),": This operation applies a window specification to a KGroupedStream and returns a TimeWindowedKStream or SessionWindowedKStream that contains the records for each key within the window. For example, if you have a KGroupedStream of user actions on a website, you can window them by time to get the actions for each user within a certain time interval.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Assume we have a KGroupedStream of user actions on a website\nKGroupedStream<String, String> actions = ...; // key: user, value: action\n\n// Define a time window of 10 seconds\nTimeWindows timeWindow = TimeWindows.of(Duration.ofSeconds(10));\n\n// Window the actions by user and time and print the result\nTimeWindowedKStream<String, String> windowed = actions.windowedBy(timeWindow);\nwindowed.foreach((windowedKey, action) -> {\n  String user = windowedKey.key();\n  Window window = windowedKey.window();\n  System.out.println(window + ": " + user + ": " + action);\n  });\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'Input:\n  actions: user1 -> "login", user2 -> "search", user1 -> "view", user3 -> "login", user2 -> "buy", user1 -> "logout"\nOutput:\n  windowed: [0s-10s] -> user1 -> ("login", "view"), user2 -> ("search"), user3 -> ("login")\n            [10s-20s] -> user2 -> ("buy"), user1 -> ("logout")\n'))))}m.isMDXComponent=!0},6398:function(e,t,a){a.r(t),a.d(t,{assets:function(){return l},contentTitle:function(){return i},default:function(){return m},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return p}});var r=a(3117),n=(a(7294),a(3905));const o={sidebar_position:1e3,sidebar_label:"Kafka Implement Cluster",title:"Kafka  Implement Cluster",tags:["Kafka Knowledge"]},i=void 0,s={unversionedId:"kafka/hero/implement-cluster",id:"kafka/hero/implement-cluster",title:"Kafka  Implement Cluster",description:"Kafka Implement Cluster",source:"@site/docs/kafka/hero/implement-cluster.md",sourceDirName:"kafka/hero",slug:"/kafka/hero/implement-cluster",permalink:"/devviews/interviews/kafka/hero/implement-cluster",draft:!1,editUrl:"https://github.com/owntuts/devviews/edit/main/docs/kafka/hero/implement-cluster.md",tags:[{label:"Kafka Knowledge",permalink:"/devviews/interviews/tags/kafka-knowledge"}],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3,sidebar_label:"Kafka Implement Cluster",title:"Kafka  Implement Cluster",tags:["Kafka Knowledge"]},sidebar:"kafkaInterviewSidebar",previous:{title:"list of common stream operations",permalink:"/devviews/interviews/kafka/hero/common-stream"},next:{title:"Kafka kstream vs ktable",permalink:"/devviews/interviews/kafka/hero/kstreamvsktable"}},l={},p=[],u={toc:p},c="wrapper";function m(e){let{components:t,...a}=e;return(0,n.kt)(c,(0,r.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("details",{open:!0},(0,n.kt)("summary",null,(0,n.kt)("h5",null,"Kafka Implement Cluster")),(0,n.kt)("p",null,"Starting a Kafka cluster, create a topic, and use the console producer and consumer to send and receive messages. Here are the steps:"),(0,n.kt)("p",null,"Download and extract Kafka from ",(0,n.kt)("a",{parentName:"p",href:"https://kafka.apache.org/downloads"},"https://kafka.apache.org/downloads")),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Start ZooKeeper:"))),(0,n.kt)("p",null,"In a terminal window, type the following command to start ZooKeeper using the default ZooKeeper configuration file:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sh"},"$ bin/zookeeper-server-start.sh config/zookeeper.properties\n")),(0,n.kt)("ol",{start:2},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Start Kafka-Broker server:"))),(0,n.kt)("p",null,"In a new terminal window, type the following command to start a Kafka broker server on Port 9092:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sh"},"$ bin/kafka-server-start.sh config/server.properties\n")),(0,n.kt)("ol",{start:3},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Create a Kafka Topic:"))),(0,n.kt)("p",null,'Create a new Kafka topic called "my-topic" with three partitions and a replication factor of three, using the following command:'),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sh"},"# contact to a brocker\nbin/kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --replication-factor 3 --partitions 3\n")),(0,n.kt)("p",null,"or"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sh"},"# contact to zookeeper directly (not recommended)\n$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 3 --topic my-topic\n")),(0,n.kt)("ol",{start:4},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Start Kafka Consumer:"))),(0,n.kt)("p",null,'Start a Kafka console consumer to consume messages from "my-topic":'),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sh"},"$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning\n")),(0,n.kt)("ol",{start:5},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Start Kafka Producer:"))),(0,n.kt)("p",null,'In a new terminal window, start a Kafka console producer that will send messages to "my-topic":'),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sh"},"$ bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic my-topic\n")),(0,n.kt)("ol",{start:6},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Send message to kafka topic"))),(0,n.kt)("p",null,"By default, ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"each line you enter will result in a separate message being sent to the topic")),". If you want to send messages with keys and values, you need to specify two additional properties: parse.key and key.separator. For example, to send messages with keys and values separated by a colon (:), you can run the following command:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kafka-console-producer.sh --topic my-topic --bootstrap-server localhost:9092 --property parse.key=true --property key.separator=:\n")),(0,n.kt)("p",null,"Then you can enter messages like this:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"key1:value1\nkey2:value2\nkey3:value3\n")),(0,n.kt)("p",null,":::note"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("inlineCode",{parentName:"p"},"--bootstrap-server localhost:9092")," is a parameter that ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"tells where to find one or more Kafka brokers"))," in the cluster. The bootstrap server does not need to be a leader. You can specify multiple bootstrap servers separated by commas, such as ",(0,n.kt)("inlineCode",{parentName:"p"},"--bootstrap-server localhost:9092,localhost:9093")," (This provides redundancy in case one of the servers is down or unreachable). After contacting to the brocker, it will contact to Zookepper to request info about leader to do the job. ")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"If you are ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"running Kafka on a different machine or network")),", you need to replace localhost with the hostname or IP address of the Kafka broker.")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("inlineCode",{parentName:"p"},"--broker-list localhost:9092")," is deprecated version of ",(0,n.kt)("inlineCode",{parentName:"p"},"--bootstrap-server localhost:9092"),"."))),(0,n.kt)("p",null,":::")))}m.isMDXComponent=!0},5008:function(e,t,a){a.r(t),a.d(t,{assets:function(){return l},contentTitle:function(){return i},default:function(){return m},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return p}});var r=a(3117),n=(a(7294),a(3905));const o={sidebar_position:1e3,sidebar_label:"Kafka kstream vs ktable",title:"Kafka kstream vs ktable",tags:["Kafka Knowledge"]},i=void 0,s={unversionedId:"kafka/hero/kstreamvsktable",id:"kafka/hero/kstreamvsktable",title:"Kafka kstream vs ktable",description:"kstreamvsktable.md",source:"@site/docs/kafka/hero/kstreamvsktable.md",sourceDirName:"kafka/hero",slug:"/kafka/hero/kstreamvsktable",permalink:"/devviews/interviews/kafka/hero/kstreamvsktable",draft:!1,editUrl:"https://github.com/owntuts/devviews/edit/main/docs/kafka/hero/kstreamvsktable.md",tags:[{label:"Kafka Knowledge",permalink:"/devviews/interviews/tags/kafka-knowledge"}],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3,sidebar_label:"Kafka kstream vs ktable",title:"Kafka kstream vs ktable",tags:["Kafka Knowledge"]},sidebar:"kafkaInterviewSidebar",previous:{title:"Kafka Implement Cluster",permalink:"/devviews/interviews/kafka/hero/implement-cluster"},next:{title:"Kafka Partition",permalink:"/devviews/interviews/kafka/hero/partition"}},l={},p=[],u={toc:p},c="wrapper";function m(e){let{components:t,...a}=e;return(0,n.kt)(c,(0,r.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("details",{open:!0},(0,n.kt)("summary",null,(0,n.kt)("h5",null,"kstreamvsktable.md")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"KStream")," is used ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"for processing continuous data streams")),", such as real-time log aggregation, and converts data into a stream of keys and values. KStream supports updating the output with each new message with the same key. However, KStream does ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"not support direct querying")),", as it's meant for processing and forwarding.")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"KTable"),", on the other hand, is used for ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"maintaining the latest snapshot of data per key")),", such as aggregating data by key or maintaining a session window. It ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"uses a local state store to enable lookups and joins")),", and stores the latest value per key as change updates. KTable supports direct querying of the latest value per key."))),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"Feature"),(0,n.kt)("th",{parentName:"tr",align:null},"KStream"),(0,n.kt)("th",{parentName:"tr",align:null},"KTable"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"Data type"),(0,n.kt)("td",{parentName:"tr",align:null},"Stream of keys and values"),(0,n.kt)("td",{parentName:"tr",align:null},"Table of keys and values")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"Update semantics"),(0,n.kt)("td",{parentName:"tr",align:null},"Each new message with same key results in updated output"),(0,n.kt)("td",{parentName:"tr",align:null},"Latest value per key will be stored as change updates")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"State store"),(0,n.kt)("td",{parentName:"tr",align:null},"Not possible to leverage state store"),(0,n.kt)("td",{parentName:"tr",align:null},"Uses local state store to enable lookups and joins")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"Querying"),(0,n.kt)("td",{parentName:"tr",align:null},"No direct querying \u2013 meant for processing and forwarding"),(0,n.kt)("td",{parentName:"tr",align:null},"Direct querying of the latest value per key")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"Use case"),(0,n.kt)("td",{parentName:"tr",align:null},"Real-time data processing and event-driven applications"),(0,n.kt)("td",{parentName:"tr",align:null},"Maintaining the latest snapshot of data per key")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},"Example"),(0,n.kt)("td",{parentName:"tr",align:null},"Continuous data streams, real-time log aggregation"),(0,n.kt)("td",{parentName:"tr",align:null},"Aggregating data by key, maintaining a session window, etc.")))),(0,n.kt)("p",null,"In summary, KStream is suitable for real-time data processing and event-driven applications, while KTable is used for maintaining the latest snapshot of data per key that can be queried.")))}m.isMDXComponent=!0},1145:function(e,t,a){a.r(t),a.d(t,{assets:function(){return l},contentTitle:function(){return i},default:function(){return m},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return p}});var r=a(3117),n=(a(7294),a(3905));const o={sidebar_position:1e3,sidebar_label:"Kafka Partition",title:"Kafka Partition",tags:["Kafka Knowledge"]},i=void 0,s={unversionedId:"kafka/hero/partition",id:"kafka/hero/partition",title:"Kafka Partition",description:"Topic & Partitioning in Kafka",source:"@site/docs/kafka/hero/partition.md",sourceDirName:"kafka/hero",slug:"/kafka/hero/partition",permalink:"/devviews/interviews/kafka/hero/partition",draft:!1,editUrl:"https://github.com/owntuts/devviews/edit/main/docs/kafka/hero/partition.md",tags:[{label:"Kafka Knowledge",permalink:"/devviews/interviews/tags/kafka-knowledge"}],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3,sidebar_label:"Kafka Partition",title:"Kafka Partition",tags:["Kafka Knowledge"]},sidebar:"kafkaInterviewSidebar",previous:{title:"Kafka kstream vs ktable",permalink:"/devviews/interviews/kafka/hero/kstreamvsktable"},next:{title:"Kafka Pub/Sub Example",permalink:"/devviews/interviews/kafka/hero/pub-sub-strem-example"}},l={},p=[{value:"How it works",id:"how-it-works",level:3}],u={toc:p},c="wrapper";function m(e){let{components:t,...o}=e;return(0,n.kt)(c,(0,r.Z)({},u,o,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("details",{open:!0},(0,n.kt)("summary",null,(0,n.kt)("h5",null,"Topic & Partitioning in Kafka")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"A topic")," is like ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"a log of events")),". "),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"First, they are ",(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("em",{parentName:"strong"},"append only")),": When you write a new message into a log, it always goes on the end. "),(0,n.kt)("li",{parentName:"ul"},"Second, they can only be ",(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("em",{parentName:"strong"},"read by seeking an arbitrary offset"))," in the log, then by scanning sequential log entries. "),(0,n.kt)("li",{parentName:"ul"},"Third, events in the log are ",(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("em",{parentName:"strong"},"immutable\u2014once something has happened")),", it is exceedingly difficult to make it un-happen."))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"A partition")," is a unit of distribution and replication for topics. ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"Topics can have one or more partitions")),", each of which is essentially an ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"ordered, immutable sequence of records")),". Partitions allow for the parallelism of data ingestion from producers and parallel processing of data by consumer groups."))),(0,n.kt)("h3",{id:"how-it-works"},"How it works"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"A producer sends a message to a Kafka topic"),(0,n.kt)("li",{parentName:"ol"},"The message is ",(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("em",{parentName:"strong"},"written to the end"))," of the partition's log & also ",(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("em",{parentName:"strong"},"assigned a unique offset"))," (which is a sequential id number) that represents the message's position in the partition. "),(0,n.kt)("li",{parentName:"ol"},"Consumers can then read data from these partitions by ",(0,n.kt)("strong",{parentName:"li"},(0,n.kt)("em",{parentName:"strong"},"specifying the topic and partition, as well as the offset"))," from which they want to start reading.")),(0,n.kt)("p",null,"Here's an example to explain how partitions work in Kafka:"),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Kafka Partition",src:a(6608).Z,width:"1653",height:"545"})),(0,n.kt)("p",null,'Imagine a Kafka topic "User_Logins" with two partitions (P1, P2) and two consumers in the same consumer group. Each consumer wants to read data from the "User_Logins" topic. When a producer sends a message to the topic, it is written to one of the partitions (for example, P1). All the consumers in the same consumer group will read messages from all the partitions in the topic, but ',(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"each consumer will only read messages from one partition at a time"))," to ensure parallelism (ex, consumer group A)."),(0,n.kt)("p",null,"If the number of consumers is less than the number of partitions, ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"some consumers may fetch messages from multiple partitions"))," (ex, consumer group B). By increasing the number of partitions in a topic, it is possible to increase the throughput and parallelism of data ingestion and processing.")))}m.isMDXComponent=!0},9491:function(e,t,a){a.r(t),a.d(t,{assets:function(){return l},contentTitle:function(){return i},default:function(){return m},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return p}});var r=a(3117),n=(a(7294),a(3905));const o={sidebar_position:1e3,sidebar_label:"Kafka Pub/Sub Example",title:"Kafka Pub/Sub Example",tags:["Kafka Knowledge"]},i=void 0,s={unversionedId:"kafka/hero/pub-sub-strem-example",id:"kafka/hero/pub-sub-strem-example",title:"Kafka Pub/Sub Example",description:"Pub/Sub Example in Java",source:"@site/docs/kafka/hero/pub-sub-strem-example.md",sourceDirName:"kafka/hero",slug:"/kafka/hero/pub-sub-strem-example",permalink:"/devviews/interviews/kafka/hero/pub-sub-strem-example",draft:!1,editUrl:"https://github.com/owntuts/devviews/edit/main/docs/kafka/hero/pub-sub-strem-example.md",tags:[{label:"Kafka Knowledge",permalink:"/devviews/interviews/tags/kafka-knowledge"}],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3,sidebar_label:"Kafka Pub/Sub Example",title:"Kafka Pub/Sub Example",tags:["Kafka Knowledge"]},sidebar:"kafkaInterviewSidebar",previous:{title:"Kafka Partition",permalink:"/devviews/interviews/kafka/hero/partition"},next:{title:"Kafka Replication",permalink:"/devviews/interviews/kafka/hero/replication"}},l={},p=[],u={toc:p},c="wrapper";function m(e){let{components:t,...a}=e;return(0,n.kt)(c,(0,r.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("details",{open:!0},(0,n.kt)("summary",null,(0,n.kt)("h5",null,"Pub/Sub Example in Java")),(0,n.kt)("p",null,"Let's assume we have a Kafka topic named ",(0,n.kt)("inlineCode",{parentName:"p"},"input-topic")," that contains some text messages. We want to write a Kafka Streams application that consumes these messages, ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"counts the occurrences of words in each message, and produces the word counts"))," to another Kafka topic named ",(0,n.kt)("inlineCode",{parentName:"p"},"output-topic"),"."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Producer"),": Create a producer that sends some text messages to the input-topic.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Import the required libraries\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\n\nimport java.util.Properties;\n\n// Create a properties object with the required configuration\nProperties producerProps = new Properties();\nproducerProps.put("bootstrap.servers", "localhost:9092");\nproducerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");\nproducerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");\n\n// Create a Kafka producer with the properties\nKafkaProducer<String, String> producer = new KafkaProducer<>(producerProps);\n\n// Create some text messages as producer records\nProducerRecord<String, String> record1 = new ProducerRecord<>("input-topic", "Hello world");\nProducerRecord<String, String> record2 = new ProducerRecord<>("input-topic", "Kafka Streams is awesome");\nProducerRecord<String, String> record3 = new ProducerRecord<>("input-topic", "Baeldung is a great resource for learning");\n\n// Send the records asynchronously and handle the response\nproducer.send(record1, (metadata, exception) -> {\n  if (exception != null) {\n    // Handle the error\n    System.err.println("Failed to publish: " + exception);\n  } else {\n    // Handle the success\n    System.out.println("Published: " + metadata);\n  }\n});\nproducer.send(record2, (metadata, exception) -> {\n  if (exception != null) {\n    // Handle the error\n    System.err.println("Failed to publish: " + exception);\n  } else {\n    // Handle the success\n    System.out.println("Published: " + metadata);\n  }\n});\nproducer.send(record3, (metadata, exception) -> {\n  if (exception != null) {\n    // Handle the error\n    System.err.println("Failed to publish: " + exception);\n  } else {\n    // Handle the success\n    System.out.println("Published: " + metadata);\n  }\n});\n\n// Close the producer\nproducer.close();\n')),(0,n.kt)("ol",{start:2},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Kafka Streams"),": Create a Kafka Streams application that consumes these messages from the ",(0,n.kt)("inlineCode",{parentName:"li"},"input-topic")," and produces the word counts to the ",(0,n.kt)("inlineCode",{parentName:"li"},"output-topic"),".")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Import the required libraries\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.kstream.KStream;\n\nimport java.util.Arrays;\nimport java.util.Properties;\n\n// Create a properties object with the required configuration\nProperties streamsProps = new Properties();\nstreamsProps.put(StreamsConfig.APPLICATION_ID_CONFIG, "wordcount-app");\nstreamsProps.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");\nstreamsProps.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\nstreamsProps.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n\n// Create a StreamsBuilder object\nStreamsBuilder builder = new StreamsBuilder();\n\n// Create a stream from the input-topic\nKStream<String, String> input = builder.stream("input-topic");\n\n// Apply some operations on the stream to count words\nKStream<String, Long> output = input\n    .flatMapValues(value -> Arrays.asList(value.toLowerCase().split("\\\\W+"))) // Split each message into words\n    .groupBy((key, value) -> value) // Group records by word\n    .count() // Count the number of records per word\n    .toStream(); // Convert the result to a stream\n\n// Send the output stream to the output-topic\noutput.to("output-topic");\n\n// Create and start a KafkaStreams object\nKafkaStreams streams = new KafkaStreams(builder.build(), streamsProps);\nstreams.start();\n')),(0,n.kt)("ol",{start:3},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Consumer"),": Create a consumer that receives the word counts from the output-topic.")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-java"},'// Import the required libraries\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\n\nimport java.time.Duration;\nimport java.util.Collections;\nimport java.util.Properties;\n\n// Create a properties object with the required configuration\nProperties consumerProps = new Properties();\nconsumerProps.put("bootstrap.servers", "localhost:9092");\nconsumerProps.put("group.id", "wordcount-group");\nconsumerProps.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");\nconsumerProps.put("value.deserializer", "org.apache.kafka.common.serialization.LongDeserializer");\n\n// Create a Kafka consumer with the properties\nKafkaConsumer<String, Long> consumer = new KafkaConsumer<>(consumerProps);\n\n// Subscribe to the output-topic\nconsumer.subscribe(Collections.singletonList("output-topic"));\n\n// Poll for records and print them\nwhile (true) {\n  ConsumerRecords<String, Long> records = consumer.poll(Duration.ofSeconds(1));\n  for (ConsumerRecord<String, Long> record : records) {\n    System.out.println("Word: " + record.key() + ", Count: " + record.value());\n  }\n}\n'))))}m.isMDXComponent=!0},2793:function(e,t,a){a.r(t),a.d(t,{assets:function(){return l},contentTitle:function(){return i},default:function(){return m},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return p}});var r=a(3117),n=(a(7294),a(3905));const o={sidebar_position:1e3,sidebar_label:"Kafka Replication",title:"Kafka Replication",tags:["Kafka Knowledge"]},i=void 0,s={unversionedId:"kafka/hero/replication",id:"kafka/hero/replication",title:"Kafka Replication",description:"Data Distribution & Replication in kafka",source:"@site/docs/kafka/hero/replication.md",sourceDirName:"kafka/hero",slug:"/kafka/hero/replication",permalink:"/devviews/interviews/kafka/hero/replication",draft:!1,editUrl:"https://github.com/owntuts/devviews/edit/main/docs/kafka/hero/replication.md",tags:[{label:"Kafka Knowledge",permalink:"/devviews/interviews/tags/kafka-knowledge"}],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3,sidebar_label:"Kafka Replication",title:"Kafka Replication",tags:["Kafka Knowledge"]},sidebar:"kafkaInterviewSidebar",previous:{title:"Kafka Pub/Sub Example",permalink:"/devviews/interviews/kafka/hero/pub-sub-strem-example"},next:{title:"Kafka Zookeeper Roles",permalink:"/devviews/interviews/kafka/hero/zookeeper"}},l={},p=[{value:"Here&#39;s an overview of how data distribution and replication work in Kafka:",id:"heres-an-overview-of-how-data-distribution-and-replication-work-in-kafka",level:3}],u={toc:p},c="wrapper";function m(e){let{components:t,...o}=e;return(0,n.kt)(c,(0,r.Z)({},u,o,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("details",{open:!0},(0,n.kt)("summary",null,(0,n.kt)("h5",null,"Data Distribution & Replication in kafka")),(0,n.kt)("p",null,"In Kafka, ",(0,n.kt)("strong",{parentName:"p"},"data distribution and replication")," are key features that help ensure high availability, fault-tolerance, and durability."),(0,n.kt)("p",null,"Every Kafka topic is divided into one or more partitions, and each partition is replicated across multiple Kafka brokers. This means that each partition has multiple copies, called replicas, that are stored on different brokers for redundancy and fault-tolerance."),(0,n.kt)("h3",{id:"heres-an-overview-of-how-data-distribution-and-replication-work-in-kafka"},"Here's an overview of how data distribution and replication work in Kafka:"),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Kafka Replication",src:a(4895).Z,width:"1314",height:"774"})),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Partitioning:")," When a producer ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"sends a message to a Kafka topic")),", the message is assigned to a specific partition using a hashing algorithm based on the message's key. The partition is identified by an integer value from 0 to (number of partitions - 1).")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Replicating:")," (Each partition is ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"replicated across multiple Kafka brokers")),") When a topic is created, the user sets the replication factor, which determines the number of replicas for each partition. The recommended replication factor is 3, meaning each partition has three replicas on different brokers."),(0,n.kt)("ol",{parentName:"li"},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"The replicas for each partition include a leader replica and multiple follower replicas. ",(0,n.kt)("strong",{parentName:"p"},"The leader replica")," is responsible for coordinating writes and managing the partition. ",(0,n.kt)("strong",{parentName:"p"},"The follower replicas")," replicate data from the leader and provide backup and fault-tolerance in case of a server failure.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"The leader replica receives the message from the producer and ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"writes it to the end of the partition's log")),". The message is assigned a unique offset, which represents its sequence number in the partition.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"The follower replicas replicate the messages"))," written by the leader by copying the log segment files from the leader to their own log directories. Each replica maintains an offset value, indicating the last message it has replicated from the leader. Follower replicas regularly send requests to the leader to ensure they are up-to-date and do not fall behind.")))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Data Distribution"),": When a ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"consumer group subscribes to a Kafka topic"))," and its associated partitions, each consumer is assigned to only one partition within the same topic, ensuring parallelism for data processing. ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"Consumers read messages from the partition in the order they were written")),", starting from their assigned offset.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Managment:")," In case of a broker failure, the corresponding leader replica may fail as well. In that case, the follower ",(0,n.kt)("strong",{parentName:"p"},(0,n.kt)("em",{parentName:"strong"},"ZooKeeper elect a new leader"))," and replicate the missing data from the new leader replica."))),(0,n.kt)("p",null,"By using the above process, Kafka provides durable, fault-tolerant, and scalable distributed messaging capabilities.")))}m.isMDXComponent=!0},7811:function(e,t,a){a.r(t),a.d(t,{assets:function(){return l},contentTitle:function(){return i},default:function(){return m},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return p}});var r=a(3117),n=(a(7294),a(3905));const o={sidebar_position:1e3,sidebar_label:"Kafka Zookeeper Roles",title:"Kafka Zookeeper Roles",tags:["Kafka Knowledge"]},i=void 0,s={unversionedId:"kafka/hero/zookeeper",id:"kafka/hero/zookeeper",title:"Kafka Zookeeper Roles",description:"Zookeeper Roles",source:"@site/docs/kafka/hero/zookeeper.md",sourceDirName:"kafka/hero",slug:"/kafka/hero/zookeeper",permalink:"/devviews/interviews/kafka/hero/zookeeper",draft:!1,editUrl:"https://github.com/owntuts/devviews/edit/main/docs/kafka/hero/zookeeper.md",tags:[{label:"Kafka Knowledge",permalink:"/devviews/interviews/tags/kafka-knowledge"}],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3,sidebar_label:"Kafka Zookeeper Roles",title:"Kafka Zookeeper Roles",tags:["Kafka Knowledge"]},sidebar:"kafkaInterviewSidebar",previous:{title:"Kafka Replication",permalink:"/devviews/interviews/kafka/hero/replication"},next:{title:"Kafka From Zero To Hero",permalink:"/devviews/interviews/kafka/kafka-from-zero-to-hero"}},l={},p=[],u={toc:p},c="wrapper";function m(e){let{components:t,...o}=e;return(0,n.kt)(c,(0,r.Z)({},u,o,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("details",{open:!0},(0,n.kt)("summary",null,(0,n.kt)("h5",null,"Zookeeper Roles")),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Kafka zookeeper",src:a(9201).Z,width:"785",height:"844"})),(0,n.kt)("p",null,"In Apache Kafka, ZooKeeper plays a vital role in managing and coordinating brokers and consumers. ZooKeeper provides distributed synchronization and coordination services and helps manage the Kafka cluster by performing the following tasks:"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Cluster Coordination"),": ZooKeeper is used to elect a controller node, which is responsible for managing the overall state of the Kafka cluster. It monitors the liveness of brokers and detects broker failures. If a broker fails, the controller reassigns its partitions to other brokers. ZooKeeper also helps in leader election for partitions and keeps track of the current leaders."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Configuration Management"),": ZooKeeper maintains the configuration information for Kafka cluster. When brokers start up, they connect to ZooKeeper to get the current cluster configuration, and update it when necessary."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Topic and Partition Management"),": ZooKeeper keeps track of the list of topics, the number of partitions in each topic, and the location of partition replicas. When a new topic is created or a partition is added or deleted, ZooKeeper updates the metadata and notifies the brokers."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Consumer Group Management"),": Kafka consumers use ZooKeeper to store and retrieve their own consumption state. The consumer group coordinator, which is responsible for managing Kafka consumer groups, registers itself with ZooKeeper and receives notifications of changes to the topic partitions."),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Quota Management"),": ZooKeeper manages Kafka's quota system, which limits the maximum number of messages or data size that a producer or consumer can send or fetch."),(0,n.kt)("p",null,"Without ZooKeeper, Kafka would not be able to operate as a distributed system since it plays a critical role in managing and coordinating the Kafka cluster. Overall, ZooKeeper serves as an important coordination layer for Kafka, enabling efficient and effective communication between different components of a Kafka cluster.")))}m.isMDXComponent=!0},5841:function(e,t,a){t.Z=a.p+"assets/images/architecture-e4005e7ea5122e022b2cf448c9379ac0.png"},6608:function(e,t,a){t.Z=a.p+"assets/images/partition-4d239924c1e39e22ffc74ab719ec1862.png"},4895:function(e,t,a){t.Z=a.p+"assets/images/replication-bdf4920601d911a4d9c718da880300b0.png"},9201:function(e,t,a){t.Z=a.p+"assets/images/zookeeper-c2bfb0024217fe8dc6effa732c4c1f2a.png"}}]);