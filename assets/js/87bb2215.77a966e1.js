"use strict";(self.webpackChunkinterviewdev=self.webpackChunkinterviewdev||[]).push([[2439],{3905:function(e,r,t){t.d(r,{Zo:function(){return c},kt:function(){return f}});var n=t(67294);function a(e,r,t){return r in e?Object.defineProperty(e,r,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[r]=t,e}function o(e,r){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);r&&(n=n.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var r=1;r<arguments.length;r++){var t=null!=arguments[r]?arguments[r]:{};r%2?o(Object(t),!0).forEach((function(r){a(e,r,t[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(t,r))}))}return e}function s(e,r){if(null==e)return{};var t,n,a=function(e,r){if(null==e)return{};var t,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],r.indexOf(t)>=0||(a[t]=e[t]);return a}(e,r);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],r.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var p=n.createContext({}),u=function(e){var r=n.useContext(p),t=r;return e&&(t="function"==typeof e?e(r):i(i({},r),e)),t},c=function(e){var r=u(e.components);return n.createElement(p.Provider,{value:r},e.children)},l="mdxType",m={inlineCode:"code",wrapper:function(e){var r=e.children;return n.createElement(n.Fragment,{},r)}},d=n.forwardRef((function(e,r){var t=e.components,a=e.mdxType,o=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),l=u(t),d=a,f=l["".concat(p,".").concat(d)]||l[d]||m[d]||o;return t?n.createElement(f,i(i({ref:r},c),{},{components:t})):n.createElement(f,i({ref:r},c))}));function f(e,r){var t=arguments,a=r&&r.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=d;var s={};for(var p in r)hasOwnProperty.call(r,p)&&(s[p]=r[p]);s.originalType=e,s[l]="string"==typeof e?e:a,i[1]=s;for(var u=2;u<o;u++)i[u]=t[u];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}d.displayName="MDXCreateElement"},89491:function(e,r,t){t.r(r),t.d(r,{assets:function(){return p},contentTitle:function(){return i},default:function(){return m},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return u}});var n=t(83117),a=(t(67294),t(3905));const o={sidebar_position:1e3,sidebar_label:"Kafka Pub/Sub Example",title:"Kafka Pub/Sub Example",tags:["Kafka Knowledge"]},i=void 0,s={unversionedId:"kafka/hero/pub-sub-strem-example",id:"kafka/hero/pub-sub-strem-example",title:"Kafka Pub/Sub Example",description:"Pub/Sub Example in Java",source:"@site/docs/kafka/hero/pub-sub-strem-example.md",sourceDirName:"kafka/hero",slug:"/kafka/hero/pub-sub-strem-example",permalink:"/devviews/interviews/kafka/hero/pub-sub-strem-example",draft:!1,editUrl:"https://github.com/owntuts/devviews/edit/main/docs/kafka/hero/pub-sub-strem-example.md",tags:[{label:"Kafka Knowledge",permalink:"/devviews/interviews/tags/kafka-knowledge"}],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3,sidebar_label:"Kafka Pub/Sub Example",title:"Kafka Pub/Sub Example",tags:["Kafka Knowledge"]},sidebar:"kafkaInterviewSidebar",previous:{title:"Kafka Partition",permalink:"/devviews/interviews/kafka/hero/partition"},next:{title:"Kafka Replication",permalink:"/devviews/interviews/kafka/hero/replication"}},p={},u=[],c={toc:u},l="wrapper";function m(e){let{components:r,...t}=e;return(0,a.kt)(l,(0,n.Z)({},c,t,{components:r,mdxType:"MDXLayout"}),(0,a.kt)("details",{open:!0},(0,a.kt)("summary",null,(0,a.kt)("h5",null,"Pub/Sub Example in Java")),(0,a.kt)("p",null,"Let's assume we have a Kafka topic named ",(0,a.kt)("inlineCode",{parentName:"p"},"input-topic")," that contains some text messages. We want to write a Kafka Streams application that consumes these messages, ",(0,a.kt)("strong",{parentName:"p"},(0,a.kt)("em",{parentName:"strong"},"counts the occurrences of words in each message, and produces the word counts"))," to another Kafka topic named ",(0,a.kt)("inlineCode",{parentName:"p"},"output-topic"),"."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Producer"),": Create a producer that sends some text messages to the input-topic.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},'// Import the required libraries\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\n\nimport java.util.Properties;\n\n// Create a properties object with the required configuration\nProperties producerProps = new Properties();\nproducerProps.put("bootstrap.servers", "localhost:9092");\nproducerProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");\nproducerProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");\n\n// Create a Kafka producer with the properties\nKafkaProducer<String, String> producer = new KafkaProducer<>(producerProps);\n\n// Create some text messages as producer records\nProducerRecord<String, String> record1 = new ProducerRecord<>("input-topic", "Hello world");\nProducerRecord<String, String> record2 = new ProducerRecord<>("input-topic", "Kafka Streams is awesome");\nProducerRecord<String, String> record3 = new ProducerRecord<>("input-topic", "Baeldung is a great resource for learning");\n\n// Send the records asynchronously and handle the response\nproducer.send(record1, (metadata, exception) -> {\n  if (exception != null) {\n    // Handle the error\n    System.err.println("Failed to publish: " + exception);\n  } else {\n    // Handle the success\n    System.out.println("Published: " + metadata);\n  }\n});\nproducer.send(record2, (metadata, exception) -> {\n  if (exception != null) {\n    // Handle the error\n    System.err.println("Failed to publish: " + exception);\n  } else {\n    // Handle the success\n    System.out.println("Published: " + metadata);\n  }\n});\nproducer.send(record3, (metadata, exception) -> {\n  if (exception != null) {\n    // Handle the error\n    System.err.println("Failed to publish: " + exception);\n  } else {\n    // Handle the success\n    System.out.println("Published: " + metadata);\n  }\n});\n\n// Close the producer\nproducer.close();\n')),(0,a.kt)("ol",{start:2},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Kafka Streams"),": Create a Kafka Streams application that consumes these messages from the ",(0,a.kt)("inlineCode",{parentName:"li"},"input-topic")," and produces the word counts to the ",(0,a.kt)("inlineCode",{parentName:"li"},"output-topic"),".")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},'// Import the required libraries\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.kstream.KStream;\n\nimport java.util.Arrays;\nimport java.util.Properties;\n\n// Create a properties object with the required configuration\nProperties streamsProps = new Properties();\nstreamsProps.put(StreamsConfig.APPLICATION_ID_CONFIG, "wordcount-app");\nstreamsProps.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");\nstreamsProps.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\nstreamsProps.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n\n// Create a StreamsBuilder object\nStreamsBuilder builder = new StreamsBuilder();\n\n// Create a stream from the input-topic\nKStream<String, String> input = builder.stream("input-topic");\n\n// Apply some operations on the stream to count words\nKStream<String, Long> output = input\n    .flatMapValues(value -> Arrays.asList(value.toLowerCase().split("\\\\W+"))) // Split each message into words\n    .groupBy((key, value) -> value) // Group records by word\n    .count() // Count the number of records per word\n    .toStream(); // Convert the result to a stream\n\n// Send the output stream to the output-topic\noutput.to("output-topic");\n\n// Create and start a KafkaStreams object\nKafkaStreams streams = new KafkaStreams(builder.build(), streamsProps);\nstreams.start();\n')),(0,a.kt)("ol",{start:3},(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Consumer"),": Create a consumer that receives the word counts from the output-topic.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-java"},'// Import the required libraries\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\n\nimport java.time.Duration;\nimport java.util.Collections;\nimport java.util.Properties;\n\n// Create a properties object with the required configuration\nProperties consumerProps = new Properties();\nconsumerProps.put("bootstrap.servers", "localhost:9092");\nconsumerProps.put("group.id", "wordcount-group");\nconsumerProps.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");\nconsumerProps.put("value.deserializer", "org.apache.kafka.common.serialization.LongDeserializer");\n\n// Create a Kafka consumer with the properties\nKafkaConsumer<String, Long> consumer = new KafkaConsumer<>(consumerProps);\n\n// Subscribe to the output-topic\nconsumer.subscribe(Collections.singletonList("output-topic"));\n\n// Poll for records and print them\nwhile (true) {\n  ConsumerRecords<String, Long> records = consumer.poll(Duration.ofSeconds(1));\n  for (ConsumerRecord<String, Long> record : records) {\n    System.out.println("Word: " + record.key() + ", Count: " + record.value());\n  }\n}\n'))))}m.isMDXComponent=!0}}]);